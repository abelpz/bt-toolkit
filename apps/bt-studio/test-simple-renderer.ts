#!/usr/bin/env ts-node

/**
 * Simple Test for Enhanced USFMRenderer
 * Tests the renderer structure without complex dependencies
 */

import * as fs from 'fs';
import * as path from 'path';

// @ts-ignore - usfm-js doesn't have type declarations
import * as usfm from 'usfm-js';

// Simplified types for testing
interface WordToken {
  uniqueId: string;
  content: string;
  occurrence: number;
  totalOccurrences: number;
  verseRef: string;
  position: { start: number; end: number; wordIndex: number };
  alignment?: {
    sourceWordId: string;
    sourceContent: string;
    sourceOccurrence: number;
    strong: string;
    lemma: string;
    morph?: string;
    alignmentGroupId?: string;
  };
  isHighlightable: boolean;
  type: 'word' | 'text' | 'punctuation';
}

interface AlignmentGroup {
  groupId: string;
  strong: string;
  lemma: string;
  sourceWord: string;
  verseRef: string;
  instances: { tokenId: string; text: string; position: number; occurrence: number }[];
  totalInstances: number;
}

interface ProcessedVerse {
  number: number;
  text: string;
  reference: string;
  wordTokens?: WordToken[];
  alignmentGroups?: AlignmentGroup[];
}

interface ProcessedParagraph {
  id: string;
  type: 'paragraph' | 'quote';
  combinedText: string;
  tokenizedContent: WordToken[];
  verses: ProcessedVerse[];
}

interface ProcessedChapter {
  number: number;
  verseCount: number;
  paragraphCount: number;
  verses: ProcessedVerse[];
  paragraphs: ProcessedParagraph[];
}

interface ProcessedScripture {
  book: string;
  bookCode: string;
  chapters: ProcessedChapter[];
}

/**
 * Simple USFM processor for testing renderer
 */
function processUSFMForRenderer(usfmContent: string, bookCode: string): ProcessedScripture {
  console.log('üîÑ Processing USFM for renderer test...');
  
  const usfmJson = usfm.toJSON(usfmContent);
  const chapters: ProcessedChapter[] = [];
  
  for (const [chapterNum, chapterData] of Object.entries(usfmJson.chapters || {})) {
    const chapter: ProcessedChapter = {
      number: parseInt(chapterNum),
      verseCount: 0,
      paragraphCount: 0,
      verses: [],
      paragraphs: []
    };
    
    // Process verses
    for (const [verseNum, verseData] of Object.entries(chapterData as any)) {
      const verse = verseData as any;
      if (verseNum === 'front' || !verse.verseObjects) continue;
      
      const verseRef = `${bookCode} ${chapterNum}:${verseNum}`;
      const { text, tokens, alignmentGroups } = extractTokensFromVerse(verse.verseObjects, verseRef);
      
      const processedVerse: ProcessedVerse = {
        number: parseInt(verseNum),
        text,
        reference: verseRef,
        wordTokens: tokens,
        alignmentGroups
      };
      
      chapter.verses.push(processedVerse);
      chapter.verseCount++;
    }
    
    // Create a simple paragraph containing all verses
    if (chapter.verses.length > 0) {
      const allTokens = chapter.verses.flatMap(v => v.wordTokens || []);
      const combinedText = chapter.verses.map(v => v.text).join(' ');
      
      const paragraph: ProcessedParagraph = {
        id: `chapter-${chapterNum}-paragraph-1`,
        type: 'paragraph',
        combinedText,
        tokenizedContent: allTokens,
        verses: chapter.verses
      };
      
      chapter.paragraphs.push(paragraph);
      chapter.paragraphCount = 1;
    }
    
    chapters.push(chapter);
  }
  
  return {
    book: 'Test Book',
    bookCode,
    chapters
  };
}

/**
 * Extract tokens from verse objects (simplified version)
 */
function extractTokensFromVerse(verseObjects: any[], verseRef: string): { 
  text: string; 
  tokens: WordToken[]; 
  alignmentGroups: AlignmentGroup[] 
} {
  const tokens: WordToken[] = [];
  const alignmentGroups: AlignmentGroup[] = [];
  let text = '';
  let wordIndex = 0;
  
  // Track word occurrences
  const wordOccurrences = new Map<string, number>();
  const wordTotalCounts = new Map<string, number>();
  
  // First pass: count occurrences
  const countWords = (objects: any[]) => {
    for (const obj of objects) {
      if (obj.type === 'word' && obj.tag === 'w') {
        const content = obj.text;
        wordTotalCounts.set(content, (wordTotalCounts.get(content) || 0) + 1);
      } else if (obj.type === 'milestone' && obj.tag === 'zaln' && obj.children) {
        countWords(obj.children);
      }
    }
  };
  countWords(verseObjects);
  
  // Track alignment groups
  const alignmentGroupMap = new Map<string, AlignmentGroup>();

  const processObject = (obj: any, alignmentContext?: any): void => {
    if (obj.type === 'text') {
      const startPos = text.length;
      text += obj.text;
      const endPos = text.length;

      // Only create tokens for non-whitespace text objects
      if (obj.text.trim()) {
        const content = obj.text;
        const uniqueId = `${verseRef}:text-${wordIndex}:1`;
        
        // Determine token type
        let tokenType: 'word' | 'text' | 'punctuation';
        if (/^[a-zA-Z]+$/.test(content.trim())) {
          tokenType = 'word'; // Pure letters
        } else {
          tokenType = 'punctuation'; // Everything else (punctuation, mixed content)
        }
        
        tokens.push({
          uniqueId,
          content,
          occurrence: 1,
          totalOccurrences: 1,
          verseRef,
          position: { start: startPos, end: endPos, wordIndex: wordIndex++ },
          isHighlightable: tokenType === 'word',
          type: tokenType
        });
      }
      // Whitespace is preserved in the text but doesn't get a token
    } else if (obj.type === 'word' && obj.tag === 'w') {
      const startPos = text.length;
      text += obj.text;
      const endPos = text.length;

      const content = obj.text;
      const currentCount = wordOccurrences.get(content) || 0;
      wordOccurrences.set(content, currentCount + 1);
      const occurrence = currentCount + 1;
      const totalOccurrences = wordTotalCounts.get(content) || 1;
      
      const uniqueId = `${verseRef}:${content}:${occurrence}`;

      const token: WordToken = {
        uniqueId,
        content,
        occurrence,
        totalOccurrences,
        verseRef,
        position: { start: startPos, end: endPos, wordIndex: wordIndex++ },
        isHighlightable: true,
        type: 'word'
      };

      // Add alignment data if in alignment context
      if (alignmentContext) {
        const strong = alignmentContext['x-strong'] || alignmentContext.strong || '';
        const lemma = alignmentContext['x-lemma'] || alignmentContext.lemma || '';
        const sourceContent = alignmentContext['x-content'] || alignmentContext.content || '';
        const sourceOccurrence = parseInt(alignmentContext['x-occurrence'] || alignmentContext.occurrence || '1');
        
        const sourceWordId = `${verseRef}:${sourceContent}:${sourceOccurrence}`;
        const groupKey = `${strong}:${lemma}:${sourceContent}`;
        
        // Get or create alignment group
        let alignmentGroup = alignmentGroupMap.get(groupKey);
        if (!alignmentGroup) {
          alignmentGroup = {
            groupId: `${verseRef}-group-${alignmentGroupMap.size + 1}`,
            strong,
            lemma,
            sourceWord: sourceContent,
            verseRef,
            instances: [],
            totalInstances: 0
          };
          alignmentGroupMap.set(groupKey, alignmentGroup);
        }
        
        alignmentGroup.totalInstances++;
        
        token.alignment = {
          sourceWordId,
          sourceContent,
          sourceOccurrence,
          strong,
          lemma,
          morph: alignmentContext['x-morph'] || alignmentContext.morph,
          alignmentGroupId: alignmentGroup.groupId
        };

        alignmentGroup.instances.push({
          tokenId: token.uniqueId,
          text: obj.text,
          position: wordIndex - 1,
          occurrence: alignmentGroup.totalInstances
        });
      }

      tokens.push(token);
    } else if (obj.type === 'milestone' && obj.tag === 'zaln') {
      if (obj.children) {
        for (const child of obj.children) {
          processObject(child, obj);
        }
      }
    }
  };

  for (const verseObj of verseObjects) {
    processObject(verseObj);
  }

  alignmentGroups.push(...Array.from(alignmentGroupMap.values()));

  return { text, tokens, alignmentGroups };
}

/**
 * Test the renderer structure
 */
function testRendererStructure() {
  console.log('üöÄ Testing Enhanced USFMRenderer Structure\n');
  
  // Read the test USFM file
  const usfmPath = path.join(__dirname, 'test-non-contiguous-alignment.usfm');
  const usfmContent = fs.readFileSync(usfmPath, 'utf8');
  
  // Process USFM
  const scripture = processUSFMForRenderer(usfmContent, 'TST');
  
  console.log('üìä RENDERER STRUCTURE ANALYSIS:');
  console.log('='.repeat(40));
  
  let totalTokens = 0;
  let totalHighlightable = 0;
  let totalAlignmentGroups = 0;
  
  for (const chapter of scripture.chapters) {
    console.log(`\nüìñ Chapter ${chapter.number}:`);
    console.log(`   Verses: ${chapter.verseCount}`);
    console.log(`   Paragraphs: ${chapter.paragraphCount}`);
    
    for (const paragraph of chapter.paragraphs) {
      console.log(`   Paragraph ${paragraph.id}:`);
      console.log(`     - Combined text: "${paragraph.combinedText}"`);
      console.log(`     - Tokenized content: ${paragraph.tokenizedContent.length} tokens`);
      
      totalTokens += paragraph.tokenizedContent.length;
      totalHighlightable += paragraph.tokenizedContent.filter(t => t.isHighlightable).length;
      
      // Show sample tokens
      const highlightableTokens = paragraph.tokenizedContent.filter(t => t.isHighlightable).slice(0, 5);
      console.log(`     - Sample highlightable tokens:`);
      highlightableTokens.forEach(token => {
        console.log(`       ‚Ä¢ "${token.content}" (${token.uniqueId}) ${token.alignment ? `[${token.alignment.strong}]` : ''}`);
      });
    }
    
    for (const verse of chapter.verses) {
      if (verse.alignmentGroups) {
        totalAlignmentGroups += verse.alignmentGroups.length;
        const nonContiguous = verse.alignmentGroups.filter(g => g.totalInstances > 1);
        if (nonContiguous.length > 0) {
          console.log(`   Verse ${verse.number} non-contiguous groups:`);
          nonContiguous.forEach(group => {
            console.log(`     ‚Ä¢ ${group.sourceWord} (${group.strong}) ‚Üí [${group.instances.map(i => i.text).join(', ')}]`);
          });
        }
      }
    }
  }
  
  console.log('\nüìà SUMMARY:');
  console.log('='.repeat(15));
  console.log(`Total tokens: ${totalTokens}`);
  console.log(`Highlightable tokens: ${totalHighlightable}`);
  console.log(`Alignment groups: ${totalAlignmentGroups}`);
  
  // Generate React component usage example
  console.log('\nüé® REACT COMPONENT USAGE:');
  console.log('='.repeat(30));
  
  console.log('```tsx');
  console.log('import { USFMRenderer } from "./components/resources/USFMRenderer";');
  console.log('import type { WordToken, ProcessedVerse } from "./types/context";');
  console.log('');
  console.log('function ScriptureViewer() {');
  console.log('  const handleTokenClick = (token: WordToken, verse: ProcessedVerse) => {');
  console.log('    console.log("Token clicked:", {');
  console.log('      id: token.uniqueId,');
  console.log('      content: token.content,');
  console.log('      occurrence: `${token.occurrence}/${token.totalOccurrences}`,');
  console.log('      alignment: token.alignment');
  console.log('    });');
  console.log('    ');
  console.log('    // Highlight related words in same alignment group');
  console.log('    if (token.alignment?.alignmentGroupId) {');
  console.log('      highlightAlignmentGroup(token.alignment.alignmentGroupId);');
  console.log('    }');
  console.log('  };');
  console.log('');
  console.log('  return (');
  console.log('    <USFMRenderer');
  console.log('      scripture={processedScripture}');
  console.log('      showVerseNumbers={true}');
  console.log('      showAlignments={true}');
  console.log('      onTokenClick={handleTokenClick}');
  console.log('      highlightWords={["he", "is"]}');
  console.log('      className="enhanced-scripture"');
  console.log('    />');
  console.log('  );');
  console.log('}');
  console.log('```');
  
  // Save structure for inspection
  const outputDir = path.join(__dirname, 'test-output');
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir, { recursive: true });
  }
  
  fs.writeFileSync(
    path.join(outputDir, 'renderer-test-structure.json'),
    JSON.stringify(scripture, null, 2)
  );
  
  console.log('\n‚úÖ Structure saved to test-output/renderer-test-structure.json');
  
  // Validation
  console.log('\nüéØ VALIDATION RESULTS:');
  console.log('='.repeat(25));
  
  const checks = [
    { name: 'Chapters processed', passed: scripture.chapters.length > 0 },
    { name: 'Paragraphs have tokenized content', passed: scripture.chapters.some(ch => ch.paragraphs.some(p => p.tokenizedContent.length > 0)) },
    { name: 'Verses have word tokens', passed: scripture.chapters.some(ch => ch.verses.some(v => v.wordTokens && v.wordTokens.length > 0)) },
    { name: 'Highlightable tokens present', passed: totalHighlightable > 0 },
    { name: 'Alignment groups created', passed: totalAlignmentGroups > 0 },
    { name: 'Unique word IDs generated', passed: totalTokens > 0 }
  ];
  
  checks.forEach(check => {
    console.log(`${check.passed ? '‚úÖ' : '‚ùå'} ${check.name}`);
  });
  
  const allPassed = checks.every(check => check.passed);
  
  if (allPassed) {
    console.log('\nüéâ ALL CHECKS PASSED!');
    console.log('   ‚úì Enhanced USFMRenderer is ready');
    console.log('   ‚úì Word token system implemented');
    console.log('   ‚úì Alignment groups available');
    console.log('   ‚úì Inter-panel communication ready');
  } else {
    console.log('\n‚ö†Ô∏è  Some checks failed');
  }
  
  return scripture;
}

// Run the test
if (require.main === module) {
  testRendererStructure();
}

export { testRendererStructure };
